{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b743c17",
   "metadata": {},
   "source": [
    "# About This File\n",
    "\n",
    "I scrape climate data from [FAO's climate information tool](https://aquastat.fao.org/climate-information-tool/) for each Nigerian state in 2022, and then combine them into one dataset for the country\n",
    "\n",
    "The dataset contains the following features: \n",
    "- Month\n",
    "- Precipitation. mm/m\n",
    "- Temperature. min.℃\n",
    "- Temperature. max.℃\n",
    "- Temperature. Mean ℃\n",
    "- Rel. Hum. %\n",
    "- Sun shine. J m⁻² day⁻¹\n",
    "- Wind speed (2m) m/s\n",
    "- ETo. mm/m\n",
    "\n",
    "I also used flood occurence data from [NEMA](https://data.humdata.org/dataset/nigeria-nema-flood-affected-geographical-areasnorth-east-nigeria-flood-affected-geographical-areas/resource/833fe41d-1b92-4ca8-bfa0-8b483ed81690) on flood occurences in 2022. The features i use are:\n",
    "- State.\n",
    "- Date of occurence.\n",
    "\n",
    "Combining the two datasets and some research on favourable conditions for select crops i created a risk index which i use to evalutate PHL risk by region, crop, and month (season).\n",
    "\n",
    "The index is found by assigning weights to each variable based off how they affect a crop with floods having the highest weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4217db2d",
   "metadata": {},
   "source": [
    "## Scraping Data From [FAO](https://aquastat.fao.org/climate-information-tool/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d4a92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 1. 36 states + centroid coords\n",
    "states_coords = {\n",
    "    'Abia': (5.5320, 7.4860),\n",
    "    'FederalCapitalTerritory': (9.0765, 7.3986),\n",
    "    'Adamawa': (9.3265, 12.3984),\n",
    "    'Akwa Ibom': (5.0300, 7.9400),\n",
    "    'Anambra': (6.2109, 7.0710),\n",
    "    'Bauchi': (10.3100, 9.8400),\n",
    "    'Bayelsa': (4.6400, 6.0000),\n",
    "    'Benue': (7.3363, 8.8393),\n",
    "    'Borno': (11.8333, 13.1500),\n",
    "    'Cross River': (5.9667, 8.3333),\n",
    "    'Delta': (5.5853, 5.0184),\n",
    "    'Ebonyi': (5.7630, 7.9083),\n",
    "    'Edo': (6.6313, 5.5516),\n",
    "    'Ekiti': (7.6248, 6.8228),\n",
    "    'Enugu': (6.5207, 7.4139),\n",
    "    'Gombe': (10.2899, 11.1678),\n",
    "    'Imo': (5.5273, 7.0258),\n",
    "    'Jigawa': (12.9000, 9.5167),\n",
    "    'Kaduna': (10.5105, 7.4165),\n",
    "    'Kano': (12.0022, 8.5919),\n",
    "    'Katsina': (12.9855, 7.6088),\n",
    "    'Kebbi': (12.4500, 4.2000),\n",
    "    'Kogi': (7.7986, 6.7394),\n",
    "    'Kwara': (8.3983, 4.5414),\n",
    "    'Lagos': (6.5244, 3.3792),\n",
    "    'Nasarawa': (8.5173, 8.5167),\n",
    "    'Niger': (9.6148, 6.5564),\n",
    "    'Ogun': (7.1600, 4.2833),\n",
    "    'Ondo': (7.2500, 5.2000),\n",
    "    'Osun': (7.5000, 4.5000),\n",
    "    'Oyo': (7.8731, 3.8450),\n",
    "    'Plateau': (9.0000, 8.6775),\n",
    "    'Rivers': (4.8500, 6.5500),\n",
    "    'Sokoto': (13.0578, 5.2438),\n",
    "    'Taraba': (8.0000, 10.0000),\n",
    "    'Yobe': (12.0035, 11.8307),\n",
    "    'Zamfara': (12.1700, 6.6700),\n",
    "}\n",
    "\n",
    "# dynamically detect which states already have CSVs so we can skip.\n",
    "out_dir = Path(\"Datasets/AquaStat Data Each state\")\n",
    "exists = {\n",
    "    p.stem\n",
    "     .replace(\"_climate_2022\", \"\")\n",
    "     .replace(\"_\", \" \")\n",
    "    for p in out_dir.glob(\"*_climate_2022.csv\")\n",
    "}\n",
    "\n",
    "# 2. Selenium setup\n",
    "service = Service(\"./chromedriver-win64/chromedriver.exe\")\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "wait = WebDriverWait(driver, 30)\n",
    "\n",
    "base_url = \"https://aquastat.fao.org/climate-information-tool/complete-climate-data\"\n",
    "count = 0\n",
    "for state, (lat, lon) in states_coords.items():\n",
    "    if state in exists:\n",
    "        count += 1\n",
    "        print(f\"→ {state} already exists, skipping.\")\n",
    "        continue\n",
    "\n",
    "    url = f\"{base_url}?lat={lat}&lon={lon}&year=2022&datasource=agera5\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # optional short pause for JS boot\n",
    "    time.sleep(15)\n",
    "\n",
    "    # wait until table is present, with retry on timeout\n",
    "    try:\n",
    "        table = wait.until(EC.presence_of_element_located(\n",
    "            (By.CSS_SELECTOR, \"table.data-table-2 tbody tr\")))\n",
    "    except TimeoutException:\n",
    "\n",
    "        print(f\"⚠ Timeout loading table for {state}\")\n",
    "        driver.save_screenshot(f\"{state}_timeout.png\")\n",
    "        continue\n",
    "\n",
    "    # now fetch rows\n",
    "    rows = driver.find_elements(By.CSS_SELECTOR, \"table.data-table-2 tr\")\n",
    "\n",
    "    # parse header\n",
    "    headers = [th.text.strip() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "\n",
    "    data = []\n",
    "    for tr in rows[1:]:\n",
    "        cols = [td.text.strip().replace(\"\\n\", \" \") for td in tr.find_elements(By.TAG_NAME, (\"td\"))]\n",
    "        if cols:\n",
    "            data.append(cols)\n",
    "\n",
    "    # build DataFrame and save\n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "    df.to_csv(f\"Datasets/AquaStat Data Each state/{state.replace(' ', '_')}_climate_2022.csv\", index=False)\n",
    "    count += 1\n",
    "    print(f\"→ Saved {state}.csv\",f\"{state} {count} of 36\")\n",
    "    time.sleep(2)   # be gentle on the server\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb9f95f",
   "metadata": {},
   "source": [
    "## Aggreagting Data For Each State Into One Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a34d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# gather all per‑state CSVs\n",
    "csv_files = glob.glob(r\"Datasets\\AquaStat Data Each state\\*_climate_2022.csv\")\n",
    "dfs = []\n",
    "for fp in csv_files:\n",
    "    state = os.path.basename(fp) \\\n",
    "              .replace(\"_climate_2022.csv\", \"\") \\\n",
    "              .replace(\"_\", \" \")\n",
    "    df = pd.read_csv(fp)\n",
    "    df[\"state\"] = state\n",
    "    dfs.append(df)\n",
    "\n",
    "# concatenate\n",
    "if dfs:\n",
    "    all_states = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # drop any “Total” rows under Month\n",
    "    all_states = all_states[all_states[\"Month\"] != \"Total\"]\n",
    "\n",
    "    # write out\n",
    "    out_path = r\"Datasets/all_states_climate_2022.csv\"\n",
    "    all_states.to_csv(out_path, index=False)\n",
    "    print(f\"→ Aggregated {len(dfs)} files (minus Totals) into {out_path}\")\n",
    "else:\n",
    "    print(\"⚠ No CSV files found to aggregate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d572ea1",
   "metadata": {},
   "source": [
    "## Adding Flood Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1293992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import calendar\n",
    "\n",
    "# 1. load climate + state data\n",
    "climate_path = r\"Datasets\\all_states_climate_2022.csv\"\n",
    "climate = pd.read_csv(climate_path)\n",
    "\n",
    "# 2. load flood occurrences as strings, strip whitespace\n",
    "flood = pd.read_csv(r\"Datasets\\flood_data_2022.csv\", dtype=str)\n",
    "flood[\"raw_date\"] = flood[\"DATE OF OCCURRENCE\"].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "# 3. drop blanks and filter valid dd/mm/YYYY dates\n",
    "mask_valid = flood[\"raw_date\"].str.match(r\"^\\d{1,2}/\\d{1,2}/\\d{4}$\")\n",
    "invalid = flood[~mask_valid]\n",
    "if not invalid.empty:\n",
    "    print(\"⚠ Invalid or blank dates:\")\n",
    "    print(invalid[[\"raw_date\", \"State\"]])\n",
    "\n",
    "flood = flood[mask_valid].copy()\n",
    "\n",
    "# 4. parse to datetime\n",
    "flood[\"DATE OF OCCURRENCE\"] = pd.to_datetime(\n",
    "    flood[\"raw_date\"], dayfirst=True, format=\"%d/%m/%Y\"\n",
    ")\n",
    "\n",
    "# 5. extract month abbrev (Jan, Feb, …)\n",
    "flood[\"Month\"] = flood[\"DATE OF OCCURRENCE\"].dt.month.map(lambda m: calendar.month_abbr[m])\n",
    "\n",
    "# 6. build set of (month, state) where flood occurred\n",
    "flood_pairs = set(zip(\n",
    "    flood[\"Month\"],\n",
    "    flood[\"State\"].str.strip().str.title()\n",
    "))\n",
    "\n",
    "# 7. add flood flag to climate df\n",
    "climate[\"flood\"] = climate.apply(\n",
    "    lambda r: 1 if (r[\"Month\"], r[\"state\"]) in flood_pairs else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 8. save augmented dataset\n",
    "out_path = r\"Datasets\\all_states_climate_2022_with_flood.csv\"\n",
    "climate.to_csv(out_path, index=False)\n",
    "print(f\"→ Wrote augmented file with flood flag: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888e797c",
   "metadata": {},
   "source": [
    "## Creating Risk Index & Exporting New Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79d52c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"../Datasets/all_states_climate_2022_with_flood.csv\")\n",
    "df.columns = df.columns.str.replace('\\n', ' ').str.strip()\n",
    "df = df.rename(columns={\n",
    "    'Prc. mm/m':  'Precipitation',\n",
    "    'Tmp. min.': 'tmin',\n",
    "    'Tmp. max.': 'tmax',\n",
    "    'Tmp. Mean':'Temperature',\n",
    "    'Rel. Hum. %':'Relative Humidity',\n",
    "    'Sun shine':'Sunshine',\n",
    "    'Wind (2m) m/s':'Wind',\n",
    "    'ETo mm/m':    'Evapotranspiration',\n",
    "    'State':'state'\n",
    "})\n",
    "\n",
    "# === 1. Define crop-specific weights based on climate sensitivity ===\n",
    "crop_weights = {\n",
    "    'Maize': {\n",
    "        'Temperature': 0.25,\n",
    "        'Relative Humidity': 0.25,\n",
    "        'Precipitation': 0.15,\n",
    "        'Wind': 0.1,\n",
    "        'Evapotranspiration': 0.1\n",
    "    },\n",
    "    'Rice': {\n",
    "        'Temperature': 0.2,\n",
    "        'Relative Humidity': 0.25,\n",
    "        'Precipitation': 0.2,\n",
    "        'Wind': 0.1,\n",
    "        'Evapotranspiration': 0.1\n",
    "    },\n",
    "    'Sorghum': {\n",
    "        'Temperature': 0.2,\n",
    "        'Relative Humidity': 0.25,\n",
    "        'Precipitation': 0.2,\n",
    "        'Wind': 0.1,\n",
    "        'Evapotranspiration': 0.1\n",
    "    },\n",
    "    'Beans': {\n",
    "        'Temperature': 0.25,\n",
    "        'Relative Humidity': 0.25,\n",
    "        'Precipitation': 0.2,\n",
    "        'Wind': 0.1,\n",
    "        'Evapotranspiration': 0.1\n",
    "    },\n",
    "    'Millet': {\n",
    "        'Temperature': 0.2,\n",
    "        'Relative Humidity': 0.25,\n",
    "        'Precipitation': 0.15,\n",
    "        'Wind': 0.1,\n",
    "        'Evapotranspiration': 0.15\n",
    "    }\n",
    "}\n",
    "\n",
    "# === 2. Define ideal post-harvest climate thresholds for each crop ===\n",
    "thresholds = {\n",
    "    'Maize': {'Temperature': 15, 'Relative Humidity': 60, 'Precipitation': 10,  'Wind': 1, 'Evapotranspiration': 5},\n",
    "    'Rice': {'Temperature': 15, 'Relative Humidity': 65, 'Precipitation': 10,  'Wind': 1, 'Evapotranspiration': 5},\n",
    "    'Sorghum': {'Temperature': 15, 'Relative Humidity': 60, 'Precipitation': 10,  'Wind': 1, 'Evapotranspiration': 5},\n",
    "    'Beans': {'Temperature': 15, 'Relative Humidity': 60, 'Precipitation': 10,  'Wind': 1, 'Evapotranspiration': 5},\n",
    "    'Millet': {'Temperature': 15, 'Relative Humidity': 60, 'Precipitation': 10,  'Wind': 1, 'Evapotranspiration': 5}\n",
    "}\n",
    "\n",
    "# === 3. Normalize variables to [0, 1] ===\n",
    "vars_to_norm = set().union(*crop_weights.values())\n",
    "vars_to_norm = set(var for crop in crop_weights for var in crop_weights[crop])\n",
    "norm_data = {}\n",
    "\n",
    "for var in vars_to_norm:\n",
    "    arr = df[var].astype(float)\n",
    "    norm_data[var] = (arr - arr.min()) / (arr.max() - arr.min())\n",
    "\n",
    "norm_df = pd.DataFrame(norm_data)\n",
    "norm_df['state'] = df['state']\n",
    "norm_df['Month'] = df['Month']\n",
    "\n",
    "# === 4. Compute distance-to-threshold scores ===\n",
    "for crop in crop_weights:\n",
    "    for var in crop_weights[crop]:\n",
    "        if var not in df.columns:\n",
    "            continue\n",
    "        actual = df[var].astype(float)\n",
    "        ideal_val = thresholds[crop][var]\n",
    "        distance = abs(actual - ideal_val)\n",
    "        max_distance = distance.max() if distance.max() > 0 else 1\n",
    "        # Invert the distance to make a score between 0 (far) and 1 (ideal)\n",
    "        score = 1 - (distance / max_distance)\n",
    "        norm_df[f\"{crop}_{var}_score\"] = score\n",
    "\n",
    "# === 5. Compute final risk index for each crop ===\n",
    "for crop, weights in crop_weights.items():\n",
    "    crop_risk = 0\n",
    "    for var, weight in weights.items():\n",
    "        score_col = f\"{crop}_{var}_score\"\n",
    "        if score_col in norm_df.columns:\n",
    "            crop_risk += norm_df[score_col] * weight\n",
    "    norm_df[crop] = crop_risk\n",
    "    \n",
    "# === 5.1 Add flood impact ===\n",
    "# Merge flood status back into norm_df\n",
    "norm_df = norm_df.merge(df[['state', 'Month', 'flood']], on=['state', 'Month'], how='left')\n",
    "\n",
    "# For rows where flood == 1, add 1 to all crop risk indices\n",
    "for crop in crop_weights:\n",
    "    norm_df[crop] = norm_df[crop] + norm_df['flood']\n",
    "\n",
    "# === 6. Aggregate results by state and month ===\n",
    "risk_df = (\n",
    "    norm_df\n",
    "    .groupby(['state', 'Month'], as_index=False)[list(crop_weights.keys())]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# === 7. Preview or export results ===\n",
    "display(risk_df.tail(20))\n",
    "# risk_df.to_csv(\"crop_climate_risk_index.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493cd553",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_df[risk_df['state'] =='Kano']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_df.to_csv('../Datasets/risk_index.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
